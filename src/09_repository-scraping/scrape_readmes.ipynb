{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GitHub README Extraction \n",
    "\n",
    "During this summer, the DSGP OSS 2021 team willbe classifying GitHub repositores into different software types. To do this, we will be extracting README files from all of the OSS repos (i.e. those with OSS licenses) and then developing NLP techniques to classify those repos. In this file, we document the extraction process for GitHub README files. \n",
    "\n",
    "First, we load our packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sfs/qumulo/qhome/kb7hp/git/oss-2020/src/09_repository-scraping/\n"
     ]
    }
   ],
   "source": [
    "# load packages \n",
    "import os\n",
    "import psycopg2 as pg\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import requests as r\n",
    "import string \n",
    "import json\n",
    "import base64\n",
    "import urllib.request\n",
    "import itertools \n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "myPath = '/sfs/qumulo/qhome/kb7hp/git/oss-2020/src/09_repository-scraping/'; print(myPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will grab our data from the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zz44-b/pkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zz44-b/tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tenkjm/hw-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dasfoo/rover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dashacker/dnd5e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              slug\n",
       "0       zz44-b/pkg\n",
       "1     zz44-b/tools\n",
       "2      tenkjm/hw-8\n",
       "3     dasfoo/rover\n",
       "4  dashacker/dnd5e"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect to the database, download data \n",
    "connection = pg.connect(host = 'postgis1', database = 'sdad', \n",
    "                        user = os.environ.get('db_user'), \n",
    "                        password = os.environ.get('db_pwd'))\n",
    "\n",
    "raw_slug_data = '''SELECT slug FROM gh.repos LIMIT 100'''\n",
    "\n",
    "# convert to a dataframe, show how many missing we have (none)\n",
    "raw_slug_data = pd.read_sql_query(raw_slug_data, con=connection)\n",
    "raw_slug_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping: brandonleekramer/diversity\n",
      "Finished scraping: uva-bi-sdad/oss-2020\n",
      "Finished scraping: facebook/react\n"
     ]
    }
   ],
   "source": [
    "slugs = [\"brandonleekramer/diversity\", \"uva-bi-sdad/oss-2020\", \"facebook/react\"] #test data \n",
    "#slugs = raw_slug_data.slug.tolist()\n",
    "\n",
    "for slug in slugs:\n",
    "    url = f'https://github.com/{slug}/blob/master/README.m'\n",
    "    split_slugs = slug.split(\"/\")\n",
    "    login = split_slugs[0]\n",
    "    repo = split_slugs[1]\n",
    "    fullfilename = os.path.join('/sfs/qumulo/qhome/kb7hp/git/oss-2020/src/09_repository-scraping/', f'readme_{login}_{repo}.txt')\n",
    "    urllib.request.urlretrieve(url, fullfilename)\n",
    "    print(f'Finished scraping: {login}/{repo}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note to Crystal: \n",
    "\n",
    "The function above this note works for small-scale scraping but we need to add in the rate limit on API calls before we scale up. \n",
    "https://stackoverflow.com/questions/40748687/python-api-rate-limiting-how-to-limit-api-calls-globally\n",
    "\n",
    "We could also try to add in multiprocessing to speed things up. I'm not sure this link is the right one, but we can chat more about that.\n",
    "https://stackoverflow.com/questions/54858979/how-to-use-multiprocessing-with-requests-module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slug</th>\n",
       "      <th>readme_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brandonleekramer/diversity</td>\n",
       "      <td>The Rise of Diversity and Population Terminolo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>facebook/react</td>\n",
       "      <td>React ·    \\nReact is a JavaScript library for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uva-bi-sdad/oss-2020</td>\n",
       "      <td>UVA-BII Open Source Software 2020-21\\nAs of: 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         slug  \\\n",
       "0  brandonleekramer/diversity   \n",
       "1              facebook/react   \n",
       "2        uva-bi-sdad/oss-2020   \n",
       "\n",
       "                                         readme_text  \n",
       "0  The Rise of Diversity and Population Terminolo...  \n",
       "1  React ·    \\nReact is a JavaScript library for...  \n",
       "2  UVA-BII Open Source Software 2020-21\\nAs of: 0...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_name = []\n",
    "readme_text = [] \n",
    "for filename in os.listdir(myPath):\n",
    "    if filename.endswith('.txt'):\n",
    "        with open(os.path.join(myPath, filename)) as f:\n",
    "            content = f.read()\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            clean_html = ''.join(soup.article.findAll(text=True))\n",
    "            repo_name.append(filename)\n",
    "            readme_text.append(clean_html)\n",
    "            df = pd.DataFrame({'slug': repo_name, 'readme_text': readme_text}, columns=[\"slug\", \"readme_text\"])\n",
    "            df['slug'] = df['slug'].str.replace('readme_','')\n",
    "            df['slug'] = df['slug'].str.replace('.txt','')\n",
    "            # this works because slugs can't have underscores\n",
    "            df['slug'] = df['slug'].str.replace('_','/') \n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to write this to the database now... \n",
    "\n",
    "Try this: https://medium.com/analytics-vidhya/part-3-5-pandas-dataframe-to-postgresql-using-python-d3bc41fcf39 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brandon_env",
   "language": "python",
   "name": "brandon_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
